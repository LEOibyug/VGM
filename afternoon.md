## 会议记录：扩散模型之后视觉生成模型的发展方向 (下半场)

下午场的会议主要聚焦于工业界和初创公司的视角，探讨了扩散模型在实际应用中的演进方向，以及对下一代生成模型的思考。

### 演讲一：从推理优先视角看新的预训练范式 (New Pre-training Paradigms from an Inference-First Perspective)
**演讲者：Jiaming Song (Luma AI)**

Jiaming Song从基础设施和算法层面探讨了扩散模型之外的未来方向。他指出，Luma AI的Dream Machine在一年内实现了商业化视频生成，但用户需求已超越简单的文本到图像/视频，转向更原生的多模态生成。

**1. 多模态生成的新趋势：**
*   **交错式模型 (Interleaved Models)**：当前趋势是开发能够处理任意交错序列（如文本、图像、视频、音频）输入并输出任意交错序列的模型。这与传统的视觉语言模型（输入多模态，输出文本）或扩散模型（输入多模态，输出图像）不同。
*   **算法组合**：目前大多数交错式模型基于离散自回归、离散扩散或连续扩散的组合。例如，文本部分使用离散自回归，图像部分使用连续扩散。

**2. 连续生成模型的挑战：**
Jiaming Song提出了连续生成模型面临的“三难困境”：
*   **训练稳定性 (Training Stability)**：模型是否容易训练且不易崩溃。
*   **高质量样本 (High Quality Samples)**：生成内容的视觉质量。
*   **高效推理 (Efficient Inference)**：生成速度和计算成本。
    *   **VAE和归一化流**：训练稳定，推理高效，但样本质量通常不高。
    *   **GANs和部分蒸馏方法**：样本质量高，推理高效，但训练稳定性差。
    *   **扩散模型**：样本质量高，训练稳定，但推理效率低（需要多步）。
    *   **目标**：寻找一种能同时满足这三项指标的算法。

**3. 推理时缩放的两个轴：**
*   **序列长度缩放 (Scaling in Sequence Length)**：通过增加输入/输出的token数量来提升模型能力，如LLM中的思维链（Chain-of-Thought）。
*   **精炼步数缩放 (Scaling in Refinement Steps)**：通过在相同token集合上迭代精炼来提升质量，如扩散模型通过多步去噪。
*   **理想算法**：应能在这两个轴上同时进行高效缩放。

**4. DDIM（和扩散模型）的次优性：**
*   **DDIM的局限**：DDIM（Denoising Diffusion Implicit Models）在推理时只考虑单个时间步，模型对终点的信息感知不足。即使模型容量无限，也无法在一步内生成完美结果，因为其设计并未考虑多步推理的效率。
*   **改进思路**：通过让模型同时感知起点和终点（即同时输入两个时间步），可以显著提升推理效率。这催生了“流图”（Flow Maps）等新概念，如Consistency Trajectory Models、Shortcut Models和MeanFlow。

**5. 感应矩匹配 (Inductive Moment Matching, IMM)：**
*   **核心思想**：IMM是一种新的生成算法，不依赖于去噪分数匹配或流匹配，也不直接依赖于概率ODE。其核心在于“分布一致性”，即通过比较不同路径生成的分布来优化模型。
*   **方法**：借鉴了最大均值差异（MMD）的概念，MMD类似于GAN但使用特殊的再生核希尔伯特空间（RKHS）判别器，无需优化判别器，使得训练更稳定。
*   **优势**：
    *   单阶段训练，单一目标函数。
    *   泛化了一致性模型（一致性模型可视为IMM在单样本情况下的特例）。
    *   训练稳定，达到SOTA的少步生成效果。
    *   在增加步数时，性能仍能保持SOTA，甚至超越少步结果。
*   **目标**：IMM旨在解决连续生成模型的“三难困境”，提供训练稳定、高质量样本和高效推理的综合解决方案。

**6. 推理优先的范式**：
*   强调在设计训练算法之前，应首先分析推理算法的效率和可扩展性。一个好的推理算法应该能够同时在序列长度和精炼步数上进行高效缩放。

### 演讲二：可扩展的归一化流用于视觉生成 (Scalable Normalizing Flows for Visual Generation)
**演讲者：Jiatao Gu (Apple)**

Jiatao Gu探讨了归一化流（Normalizing Flows, NF）在2025年重新焕发活力的可能性，并将其与当前主流的自回归模型和扩散模型进行对比。

**1. 生成AI的两大主流：**
*   **自回归模型 (Autoregressive Models)**：在离散空间工作，如语言模型，逐个预测token。
*   **扩散/流匹配模型 (Diffusion/Flow-Matching Models)**：在连续空间工作，从随机噪声迭代精炼图像。

**2. 自回归模型在视觉中的挑战：**
*   **离散化损失**：将图像映射到离散token会引入信息损失。
*   **统一性不足**：将自回归和扩散模型简单组合（如Kaleido Diffusion）缺乏真正的统一性。
*   **连续空间自回归的挑战**：
    *   **强大的函数**：需要能捕捉多模态分布复杂性的函数。
    *   **训练-测试不匹配 (Train-Test Mismatch)**：训练时依赖真实数据，推理时依赖模型自身预测，导致误差累积。

**3. 归一化流 (Normalizing Flows) 的复兴：**
*   **核心原理**：NF通过可逆神经网络将数据分布直接映射到简单先验分布（如高斯噪声），其逆过程即为生成模型。它直接优化似然函数，没有中间步骤的限制。
*   **历史与挑战**：NF在2019年左右曾是SOTA，但因其对可逆性的强约束、雅可比行列式计算成本高、以及架构灵活性差等问题，在扩散模型兴起后逐渐式微。
*   **复兴的理由**：演讲者认为，通过解决这些关键问题，NF可以再次成为强大的视觉生成模型。

**4. STARFlow的关键创新：**
STARFlow（Scalable Latent Normalizing Flows for High-resolution Image Synthesis）旨在解决传统NF的扩展性问题：
*   **关键要素1：自回归流 (Autoregressive Flow)**：
    *   将自回归模型与归一化流结合。每个步骤预测均值和方差，并保持可逆性。
    *   解决了传统NF中可逆性约束和雅可比行列式计算复杂的问题。
    *   单个自回归流块能力有限，但通过堆叠多个块，可以近似任意连续密度函数，且每个块都能引入“未来信息”。
*   **关键要素2：Transformer骨干 (Transformers)**：
    *   将NF中的变换函数现代化为Transformer架构，借鉴了DiT（Diffusion Transformer）的成功经验。
*   **关键要素3：深浅层架构 (Deep-Shallow Architecture)**：
    *   受LLM启发，采用深层（处理语义信息）和浅层（处理局部细节）结合的架构。
    *   深层块处理类条件或文本条件，浅层块作为“tokenizers”处理更精细的细节。
    *   这种架构提高了性能和效率，并能与预训练LLM（如Llama3）进行微调结合。
*   **关键要素4：潜在空间归一化流 (Latent Normalizing Flows)**：
    *   借鉴LDM（Latent Diffusion Models）的思路，在VAE的潜在空间中运行NF。
    *   潜在空间更紧凑，训练更稳定，并能利用VAE解码器进行去噪。
*   **关键要素5：噪声增强训练 (Noise-augmented Training)**：
    *   传统NF的极大似然训练在连续空间中存在问题（数据流形体积为零，模型容易通过操纵雅可比行列式“作弊”）。
    *   解决方案：在数据点上添加少量固定噪声，使其具有非零体积，从而稳定训练并提高样本质量。
    *   生成干净图像：通过基于分数的去噪或利用VAE解码器进行去噪。
*   **关键要素6：带引导的推理 (Inference with Guidance)**：
    *   将扩散模型中成功的分类器自由引导（CFG）技术引入自回归流。
    *   重新推导CFG公式，使其适用于同时预测均值和方差的自回归流，尤其是在潜在空间中。

**5. 成果与展望：**
*   STARFlow在ImageNet等基准测试中取得了SOTA的似然估计和图像生成质量，且收敛速度快于DiT。
*   模型支持文本到图像生成、图像编辑和交互式图像生成。
*   **未来方向**：加速推理（目前仍慢于扩散模型）、无噪声训练、可学习噪声、扩展到更多模态（视频、3D）和真正的多模态生成。

### 演讲三：扩散模型的轻量级和重量级适应性 (Light and Heavy Adaptations of Diffusion Models)
**演讲者：Varun Jampani (Stability AI)**

Varun Jampani从实际应用角度展示了扩散模型在复杂视觉任务中的强大适应性，包括材质控制、新视角合成和4D生成。

**1. 材质控制与迁移 (Material Transfer and Control)：**
*   **Alchemist (CVPR’24)**：通过对Stable Diffusion进行简单微调，实现图像中物体材质的精确参数化控制（如透明度、粗糙度、金属度、反照率），无需显式3D建模。
*   **ZeST (ECCV’24)**：实现零样本材质迁移。给定一张输入图像和一张材质示例图，Stable Diffusion可以直接将示例材质迁移到输入图像上，无需额外微调。这通过IP-Adapter、ControlNet和Inpainting等技术组合实现。
*   **MARBLE (CVPR’25)**：在CLIP空间中实现更精细的材质重组和混合。通过选择性地将CLIP特征注入Stable Diffusion的特定模块，并利用CLIP特征的插值特性，实现材质的精确控制和混合，同时保持原始图像结构。这表明扩散模型在隐式地学习材质属性方面具有强大能力。

**2. 新视角合成 (Novel View Synthesis, NVS)：**
*   **Stable Virtual Camera (2025)**：将NVS任务转化为相机条件下的视频生成问题。
*   **核心思想**：训练一个单一模型，输入一张或几张静态场景图像和精确的3D相机路径，输出沿着该路径生成的视频。
*   **技术细节**：将相机路径编码为Plucker嵌入，并将其注入Stable Diffusion的UNet中。通过两阶段采样（先生成锚点帧，再插值中间帧），实现长视频生成和良好的插值平滑度。
*   **优势**：高生成容量、平滑插值、多功能性（支持单图、多图输入，远近视角），且无需显式3D表示。
*   **局限性**：在处理人类、动物、动态纹理等动态内容时效果不佳，主要受限于训练数据。

**3. 4D 生成 (4D Generation)：**
*   **问题设定**：从单视角视频生成多视角视频，即在时间和空间上生成一致的图像网格，最终目标是优化出4D表示（如动态NeRF）。
*   **Stable Video 4D 1.0 (ICLR’25)**：早期工作，通过视图注意力和帧注意力层，实现多视角视频生成。但存在对自遮挡不鲁棒、结果模糊等问题。
*   **Stable Video 4D 2.0 (2025)**：
    *   **改进点1：随机遮罩 (Random Masking)**：在参考多视图上引入随机遮罩，减少模型对初始参考视图的依赖，提高对自遮挡的鲁棒性。
    *   **改进点2：3D注意力 (3D Attention)**：引入能够同时关注视图和时间维度的3D注意力机制，增强时空一致性。
    *   **改进点3：数据质量提升**：通过自动过滤低质量数据（如光照差、运动少、缩放不一致），即使使用更少的数据也能获得更高质量的结果。
    *   **改进点4：更好的NeRF优化**：结合两阶段的动态NeRF优化，从生成的视频中提取高质量的4D资产。
*   **成果**：SV4D 2.0在4D生成方面取得了显著提升，能够从单视角视频生成高质量的360度新视角视频，并展现出对真实世界视频的良好泛化能力。

**4. 总结与展望：**
*   扩散模型具有强大的适应性，可以应用于材质控制、新视角合成、4D生成、重光照等多种复杂视觉任务。
*   未来方向将集中于实现更精细的控制和更好的泛化能力，以适应更复杂的真实世界场景和动态内容。

### 总结性发言：扩散模型有什么问题？ (What's Wrong with Diffusion?)
**演讲者：Arash Vahdat (NVIDIA)**

Arash Vahdat以一个反思性的问题结束了上午的会议，他认为要讨论“扩散模型之后”是什么，首先要搞清楚“扩散模型有什么问题”。

**1. 扩散模型的优点：**
*   **高质量样本**：能够生成极其逼真的图像，甚至能处理复杂场景和概念。
*   **训练稳定且可扩展**：基于回归损失，训练过程稳定，易于扩展到大规模数据和模型。
*   **核心归纳偏置**：从低频内容到高频细节的生成方式，符合图像的层次结构。
*   **强大的控制与引导**：分类器自由引导（CFG）等技术提供了强大的生成控制能力。

**2. 扩散模型的问题：**
*   **采样效率 (Sampling Efficiency)**：
    *   **生成学习三难困境 (Generative Learning Trilemma)**：Arash在ICLR 2022的论文中提出，生成模型难以同时实现：快速采样、模式覆盖/多样性、高质量样本。
        *   **GANs**：快速采样、高质量样本，但模式覆盖差。
        *   **归一化流/VAE**：快速采样、模式覆盖好，但样本质量低。
        *   **扩散模型**：高质量样本、模式覆盖好，但采样速度慢。
    *   **加速策略**：通过分布蒸馏（如DMD）或轨迹蒸馏（如一致性模型）来加速采样。
        *   **DMD**：使用反向KL散度进行分布匹配，但反向KL是模式寻求的，可能牺牲多样性。F-散度（如Jensen-Shannon）可以提供更好的多样性-质量权衡。
        *   **一致性模型**：通过学习将轨迹上所有点映射到相同数据点的函数来加速。但存在训练不稳定（小时间步去噪与大时间步生成之间的权衡）和需要截断训练的问题。
*   **训练效率 (Training Efficiency)**：
    *   **高方差训练目标**：扩散模型的训练目标是预测平均值，这导致了高方差问题，需要大批量、长时间训练和EMA等技术来稳定。
    *   **REPA的启示**：REPA通过引入特征对齐，可以看作是一种方差缩减技术，帮助去噪器学习更好的内部表示，从而加速训练。
*   **重尾分布建模 (Heavy-tail Generative Modeling)**：
    *   **问题**：扩散模型在建模数据分布的“尾部”（即极端、稀有事件）时表现不佳，例如天气数据中的极端温度或降雨。
    *   **原因**：高斯扩散过程倾向于平滑分布，难以捕捉尖锐的峰值和长尾。
    *   **解决方案**：通过改变扩散过程（而非数据分布），例如使用具有重尾特性的学生T分布（Student T distribution）进行扩散，可以显著提升模型捕捉重尾分布的能力。
*   **轨迹复杂性 (Complexity of Trajectories)**：
    *   **问题**：扩散模型中的ODE轨迹可能存在高曲率，导致离散化误差，需要更多步骤才能准确模拟。
    *   **解决方案**：
        *   **改变前向过程**：例如，在视频扩散中使用“扭曲噪声”（Warped Noise），使噪声本身具有时间相关性，从而使轨迹更平滑，并赋予模型对输入变换的等变性。
        *   **改变数据分布**：通过VAE等方法将数据映射到更平滑的潜在空间，使轨迹更易于处理。

**3. 潜在空间设计**：
*   目前仍不清楚何种潜在空间最适合扩散模型训练。虽然VAE能使轨迹平滑，但Robin Rombach的LDM强调压缩。未来的研究需要探索如何设计既能压缩又能保持等变性或语义意义的潜在空间。

**4. 总结与展望**：
*   “扩散模型之后”的下一代模型，需要保留扩散模型已有的优点（高质量、稳定训练、归纳偏置、控制引导），并弥补其不足（快速采样、高效训练、重尾建模）。
*   未来的研究方向包括：加速采样、提高训练效率、更好地捕捉重尾分布，以及探索新的前向过程和潜在空间设计。

---

通过对会议下半场的详细记录，我们可以看到工业界在扩散模型应用和优化上的实践，以及学术界在理论和算法上的深入探索。无论是对现有模型的精细化改造，还是对新范式的勇敢尝试，都预示着视觉生成领域将迎来更加激动人心的发展。

