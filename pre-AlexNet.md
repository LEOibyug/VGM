
会议中提到当前的扩散模型属于“pre-AlexNet时代”的论据，主要来自于**Kaiming He（何恺明）**的演讲。他的核心观点是，尽管扩散模型取得了巨大成功，但其**训练范式与AlexNet之前识别模型（如深度信念网络DBN、去噪自编码器DAE）的“逐层训练”范式有异曲同工之处**，而**未能实现AlexNet之后识别模型所普遍采用的“端到端训练”范式**。


### 1. AlexNet之前识别模型的“逐层训练”范式：

*   **特点**：在AlexNet（2012年）之前，深度神经网络的训练非常困难。为了解决这个问题，研究人员通常采用“逐层预训练”（layer-wise pre-training）的方法。
*   **具体做法**：
    *   首先，独立训练网络的第一层（或几层），通常使用无监督或自监督目标（如去噪自编码器，DAE）。
    *   训练完成后，固定这些层的参数。
    *   然后，再训练下一层，同样使用无监督目标。
    *   这个过程逐层进行，直到所有层都被预训练完毕。
    *   最后，再对整个网络进行端到端微调。
*   **目的**：这种方法旨在为深度网络提供一个良好的初始化，避免梯度消失/爆炸等问题，从而使整个网络更容易训练。
*   **计算图**：在训练阶段，每一层的优化是相对独立的，而不是一个统一的、端到端的计算图。

### 2. 扩散模型的“逐步训练”范式与AlexNet之前的类比：

何恺明指出，扩散模型在训练和推理阶段的计算图存在显著差异，这与AlexNet之前的逐层训练有相似之处：

*   **推理阶段（Inference-time Computation）**：
    *   扩散模型从一个随机噪声开始，通过**多步迭代**去噪，逐步生成最终图像。
    *   每一步去噪都涉及一个完整的神经网络（通常是U-Net或Transformer）。
    *   因此，推理过程是一个**多步串联**的计算图，每一步都依赖前一步的输出。
*   **训练阶段（Training-time Computation）**：
    *   扩散模型的训练目标通常是**去噪分数匹配**或**预测原始数据**。
    *   在训练时，模型在**任意一个时间步**（即噪声水平）上进行训练。它接收一个带噪声的图像，并学习预测如何去除该噪声。
    *   这个训练过程是**针对单个时间步的局部优化**，而不是针对整个多步推理过程的端到端优化。
    *   模型在训练时并不知道它在推理时将如何与其他时间步的模型实例串联起来。它只是学习在给定噪声水平下执行去噪任务。
*   **核心论据**：
    *   **训练与推理的不匹配**：扩散模型在推理时是一个多步的、串联的计算图，但在训练时，它并没有直接优化这个完整的、多步的计算图。它只是在“局部”学习去噪。
    *   **“逐步训练”的本质**：何恺明将扩散模型的这种训练方式比作“逐层训练”，即模型在每个“去噪层”（时间步）上学习一个子任务，而不是直接优化从初始噪声到最终图像的整个生成路径。
    *   **缺乏统一的端到端优化**：这意味着，扩散模型没有一个单一的、端到端的损失函数，能够直接评估和优化整个多步生成过程的质量。它依赖于每个时间步的局部去噪损失。

### 3. AlexNet之后识别模型的“端到端训练”范式：

*   **特点**：AlexNet的成功标志着深度学习进入了“端到端训练”的时代。
*   **具体做法**：
    *   整个深度神经网络作为一个整体进行训练。
    *   输入数据通过所有层进行前向传播，得到最终输出。
    *   根据最终输出与目标之间的差异计算一个**统一的损失函数**。
    *   通过**反向传播**（backpropagation）将梯度从输出层一直传播回输入层，更新所有层的参数。
*   **优势**：这种范式能够更好地优化整个系统的性能，因为所有组件都在一个统一的目标下协同工作。

### 4. 结论：扩散模型仍有进步空间

何恺明认为，扩散模型虽然强大，但其训练范式仍有改进空间。如果能找到一种类似于AlexNet那样，能够实现**真正端到端训练**的生成模型范式，即：

1.  给定一个输入（如噪声）。
2.  模型通过一个统一的、前向传播的计算图生成输出。
3.  根据最终输出计算一个单一的损失函数。
4.  通过反向传播优化整个生成过程。

那么，这将是生成模型领域的又一次重大飞跃，能够进一步提升模型的效率、稳定性和性能。MeanFlow等一步生成模型正是朝着这个方向努力，试图将多步迭代的生成过程压缩为一步，从而更接近端到端训练的理想状态。

