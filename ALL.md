## 会议总结：扩散模型之后视觉生成模型的发展方向

本次会议围绕“扩散模型之后视觉生成模型的发展方向”这一核心议题，汇聚了学术界和工业界的顶尖研究人员，共同探讨了当前主流扩散模型的局限性、经典模型的潜力、新兴范式以及未来视觉生成模型的发展趋势。会议强调了研究人员挑战“常识”的重要性，并呼吁学术界与工业界加强合作，共同推动领域进步。

### 一、扩散模型的局局限性与应对策略

会议首先明确了当前视觉生成领域的主流——扩散模型（Diffusion Models）所面临的挑战，这为后续探讨“后扩散时代”的可能方向奠定了基础。

**1. 核心局限性：**
*   **速度慢 (Too Slow)**：生成一张图像需要多步迭代采样，导致推理时间长。
*   **控制难度大 (Not Easy to Control the Generation)**：难以精确控制生成内容的特定细节，如物体位置、光照条件等。
*   **编辑难 (Not Easy to Edit)**：对已生成图像进行局部修改或属性调整（如改变材质、光照）相对复杂。
*   **可解释性低 (Not Easy to Understand Why They Output Any Given Image)**：模型内部机制复杂，难以理解为何会输出特定图像。
*   **计算成本高 (Computationally Heavy)**：无论是训练大型扩散模型还是进行高分辨率图像生成，都需要庞大的计算资源。

**2. 应对策略与工程优化 (Engineering Better Diffusion Models)：**
针对上述挑战，研究人员正从多个角度进行优化和改进：
*   **加速采样与收敛**：
    *   **损失函数重加权 (Loss Rerating)**：通过重新加权损失函数（如DDPM中的Lsimple目标）或调整时间步采样策略，使模型更关注感知上重要的去噪阶段，从而加速收敛。
    *   **蒸馏 (Distillation)**：将大型、多步的扩散模型“知识”蒸馏到更小、更快的模型中，实现一步或少步生成。例如，一致性模型（Consistency Models）和InstaFlow等旨在直接学习从噪声到数据的映射，实现单步推理。
*   **引入额外特征空间 (Expanding Feature Space)**：
    *   **多模态融合**：将扩散过程扩展到RGB像素之外的特征，如视频生成中的光流（Optical Flow）信息（Video Jam），或图像生成中的单目深度、语义图等。通过将这些特征编码并与RGB潜在空间拼接，模型能更显式地学习结构和运动，从而提升性能。
    *   **高级特征对齐 (Alignment with Higher-Level Features)**：通过添加额外的损失项（如REPA损失），鼓励模型中间层的特征与预训练视觉编码器（如DINO）的特征对齐。这种方法能显著加速收敛，但需注意过度对齐可能限制模型表达能力，需要进行早期停止。
*   **潜在空间优化 (Latent Space Component)**：
    *   **改善编码器**：研究如何训练更好的自编码器，使其生成的潜在空间更适合扩散模型的训练，例如确保潜在空间中的变化是可预测且平滑的。
    *   **联合训练**：探索将VAE与扩散模型联合训练，通过特征对齐作为中间步骤，使VAE的潜在空间更具“可扩散性”（diffusibility），从而提升整体生成性能。

### 二、经典生成模型的再审视与新机遇

会议重新审视了GANs、归一化流、自回归模型等经典生成模型在当前背景下的潜力，认为它们在当前技术背景下可能焕发新的生机，并与扩散模型形成互补或竞争关系。

**1. 生成对抗网络 (GANs)：**
*   **优势**：推理速度快（一步生成）、潜在空间可控性强、在条件生成任务（如图像翻译、超分辨率）中表现出色。
*   **挑战与最新进展**：
    *   **训练稳定性**：GANs的训练以不稳定性著称。通过引入可微分数据增强（如ADA）、使用预训练模型作为判别器（如CLIP、DINO）、多尺度判别器、以及结合蒸馏技术来提高训练稳定性。
    *   **架构搜索**：GANs需要同时优化生成器和判别器，这比单一模型（如扩散模型）的架构搜索更复杂。通过使用预训练模型初始化生成器和判别器，可以简化这一过程。
    *   **注意力机制**：GANs与自注意力层兼容性较差，需要特殊的Lipschitz约束来保持判别器稳定性。
    *   **蒸馏 GANs (GAN Distillation)**：将慢速的扩散模型蒸馏为快速的GANs，结合了扩散模型的生成质量和GANs的推理速度。例如，Pix2Pix-Turbo模型在图像翻译任务中实现了比ControlNet快50倍的速度。
*   **未来展望**：GANs在单步视频生成、无监督学习、以及作为扩散模型蒸馏目标方面仍具有独特优势。

**2. 归一化流 (Normalizing Flows) 与神经ODE (Neural ODE)：**
*   **核心理念**：将数据生成过程视为从简单噪声分布到复杂数据分布的连续变换，通过可逆的神经网络实现。神经ODE将离散的神经网络层视为连续时间上的动态系统，通过求解常微分方程（ODE）来建模数据流。
*   **流匹配 (Flow Matching)**：
    *   **概念**：流匹配是一种训练生成模型的新范式，它将生成过程定义为从噪声到数据的线性插值，并通过学习一个“速度场”（velocity field）来描述这个流。扩散模型可以被视为流匹配的一个特例。
    *   **挑战**：尽管理论上流匹配可以实现连续生成，但在实际计算中，仍然需要通过有限和（finite sum）来近似积分，这使得其在实践中仍类似于ResNet的离散更新。
*   **MeanFlow**：
    *   **创新点**：MeanFlow旨在解决流匹配中的离散近似问题，实现真正的“一步生成”。它引入了“平均速度”（Average Velocity）的概念，将生成过程的积分计算转化为可导的导数计算（通过Jacobian-vector product, JVP），从而避免了复杂的迭代求解。
    *   **训练与推理**：MeanFlow直接训练一个神经网络来预测平均速度，推理时只需一步计算即可从噪声生成图像。
    *   **性能**：在ImageNet等挑战性数据集上，MeanFlow在一步生成方面取得了显著优于现有SOTA的性能，甚至两步生成的结果也接近多步扩散模型的水平，大大缩小了多步与单步生成之间的差距。
    *   **与AlexNet的类比**：演讲者将MeanFlow的思路比作生成模型领域的“AlexNet时刻”。在识别模型中，AlexNet实现了端到端训练，取代了之前的逐层预训练。类似地，MeanFlow试图将生成模型从“逐步去噪”的范式（类似于逐层训练）推向“一步到位”的端到端生成，即直接学习从噪声到最终图像的映射，并直接优化最终输出。

**3. 自回归模型 (Autoregressive Models)：**
*   **核心理念**：将图像视为序列，逐个像素或token生成，借鉴语言模型的成功经验。
*   **挑战**：
    *   **离散化损失**：将图像转换为离散token会引入信息损失。
    *   **扫描顺序**：图像的2D本质使得扫描顺序（如行主序、螺旋序）的选择成为挑战。
    *   **信息密度**：图像信息密度低于文本，导致大量冗余。
    *   **训练-测试不匹配**：训练时使用教师强制（ground truth），推理时依赖自身预测，导致误差累积。
*   **STARFlow (Scalable Latent Normalizing Flows for High-resolution Image Synthesis)**：
    *   **创新点**：将自回归模型与归一化流结合，在潜在空间进行生成。
    *   **自回归流**：每个步骤预测均值和方差，并保持可逆性，通过堆叠多个块近似任意连续密度函数。
    *   **Transformer骨干**：采用Transformer架构进行序列建模。
    *   **深浅层架构**：结合深层（处理语义）和浅层（处理细节）模块，提高性能和效率。
    *   **噪声增强训练**：通过在数据点上添加少量固定噪声，解决传统NF在连续空间训练中的不稳定性问题。
    *   **带引导的推理**：将CFG技术引入自回归流，并重新推导适用于潜在空间的引导公式。
*   **成果与展望**：STARFlow在似然估计和图像生成质量上取得SOTA，收敛速度快于DiT，支持文本到图像生成、图像编辑和交互式图像生成。未来将扩展到更多模态和无噪声训练。

### 三、新兴范式与未来展望

会议展望了超越传统图像生成的更广阔应用场景和新兴研究方向。

**1. 物理属性与可控性 (Physical Object Intrinsics & Controllability)：**
*   **核心思想**：超越像素层面的生成，深入理解并建模物理世界中物体的内在属性（如几何、材质、光照、运动、物理交互），从而实现更高级别的控制和交互。
*   **两种实现路径**：
    *   **内禀属性作为归纳偏置 (Intrinsics as Inductive Bias for SSL)**：将可微分渲染器等物理模拟器集成到生成模型的训练循环中，学习并解耦图像中隐含的几何、材质、光照等内禀属性。
    *   **内禀属性作为蒸馏目标 (Intrinsics as Distilling Targets for FMs)**：利用现有强大的视觉生成模型（如视频扩散模型）的知识，通过蒸馏等方式提取其隐含的物理属性，实现对物体生长过程、物理交互的模拟。
*   **场景语言 (Scene Language)**：提出一种更具结构化和语义化的场景表示方法，通过可编程的函数和组件来描述场景，实现更灵活的场景生成和编辑。

**2. 视觉语言模型 (VLM) 的深度融合：**
*   **核心思想**：结合大型语言模型（LLM）强大的语义理解、推理和规划能力，与视觉生成模型（特别是扩散模型）的内容生成能力，构建更智能、更具创造力的生成系统。
*   **WonderJourney/WonderWorld**：利用LLM生成长序列的场景描述，指导文本到图像生成模型逐步构建一个无限可扩展的3D视觉场景。用户可以通过语言提示和交互进行导航和编辑。VLM还可用于验证生成场景的语义一致性。
*   **语言作为视觉格式 (Language as a Visual Format)**：
    *   **颠覆性观点**：挑战像素作为唯一视觉表示的传统观念，提出将语言本身视为一种一等公民的视觉表示形式。
    *   **理论依据**：随着大型语言模型和大型视觉模型规模的增长，它们对世界的表示趋于收敛。
    *   **Cycle Consistency as Reward**：提出利用图像-文本循环一致性作为奖励函数，无需人工标注即可训练出更具视觉描述性的图像字幕模型。这使得语言能够更精确地捕捉图像细节，从而成为像素的有效替代或补充。

**3. 多模态生成 (Multimodality)：**
*   **音视频结合**：将音频、视频和文本结合，生成更具沉浸感的体验，例如为视频生成匹配的音频。
*   **3D场景与世界生成**：实时生成可交互的3D场景和世界，实现用户在虚拟世界中的自由漫游和控制，并赋予模型对3D世界的内在理解。
*   **4D 生成 (4D Generation)**：从单视角视频生成多视角视频，即在时间和空间上生成一致的图像网格，最终目标是优化出4D表示（如动态NeRF）。Stable Video 4D 2.0通过随机遮罩、3D注意力、数据质量提升和更好的NeRF优化，显著提升了4D生成效果。

**4. 模型可控性与一致性 (Controllability & Consistency)：**
*   **Flux One Context**：提出一种基于扩散模型的上下文图像生成和编辑模型，通过将数据和条件（文本、上下文图像表示）序列拼接，实现图像内容和风格的灵活修改，同时保持角色身份的一致性。
*   **新视角合成 (Novel View Synthesis, NVS)**：Stable Virtual Camera将NVS任务转化为相机条件下的视频生成问题，通过训练单一模型，从少量图像和精确相机路径生成高质量、多视角的视频。

**5. 离散流模型中的不连续性 (Discontinuities in Flow Models)：**
*   **核心思想**：在流模型中引入“跳跃”（jumps）和“分支”（branching factors），以处理不连续性、插入、删除和维度变化。
*   **Generator Matching**：将流匹配泛化到更一般的马尔可夫过程，允许模型在生成过程中进行非局部跳跃，从而更高效地探索状态空间。
*   **离散流匹配 (Discrete Flow Matching)**：将跳跃过程应用于离散空间，通过学习未归一化的分布来指导跳跃，解决传统离散扩散模型中单token修改的局限性。
*   **编辑流 (Edit Flows)**：将维度变化（插入和删除）引入流模型，实现可变长度序列的生成和编辑，如文本生成和代码生成。

### 四、会议核心观点与启示

1.  **挑战“常识”与开放心态**：研究人员应始终保持批判性思维，不盲从现有主流，敢于质疑和挑战现有模型的“常识”和局限性。
2.  **多模态与多学科融合**：视觉生成模型的未来发展将是多模态、多学科交叉融合的。将视觉、语言、音频、物理等多种模态结合，并借鉴计算机图形学、物理学、认知科学等领域的知识，将是突破现有瓶颈的关键。
3.  **可控性与可解释性是核心**：提高模型的精确控制能力和行为可解释性是关键，这将使得生成模型更具实用价值和用户友好性。
4.  **学术界与工业界共生**：鉴于当前模型训练和部署所需的巨大计算资源，学术界与工业界的紧密合作模式（如联合实验室、实习项目）将成为主流。
5.  **平衡质量与效率**：在追求极致生成质量的同时，必须兼顾模型的推理速度和计算成本。未来的模型将更加注重在质量、速度和资源消耗之间找到最佳平衡点。
6.  **超越像素的表示**：积极探索像素之外的视觉信息表示形式，如语言、物理属性等，这可能带来全新的生成范式和应用场景。
7.  **回归与创新**：在追求前沿技术的同时，不忘审视和借鉴经典模型中的宝贵思想。通过将经典方法与最新技术（如预训练模型、注意力机制）相结合，可以焕发新的生命力。
8.  **端到端训练的愿景**：简化复杂的训练流程，实现更简洁、高效的端到端生成模型训练范式，类似于识别模型领域AlexNet带来的变革。
9.  **人类智能的启发**：人类视觉系统在效率、鲁棒性和低功耗方面的特点，为未来AI模型的设计提供了重要启发。经济压力将推动模型向更高效、更节能的方向发展。

本次会议不仅展示了视觉生成模型领域的最新技术突破，更重要的是，它为领域未来的发展描绘了一幅多元、开放、充满挑战和机遇的宏伟蓝图。

