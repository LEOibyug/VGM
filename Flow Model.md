
### 1. 什么是“流”（Flow）？

在本次研讨的语境中，“流”（Flow）通常指的是**数据分布在连续时间或空间上的动态演变过程**。它将生成问题视为一个**从简单分布（如高斯噪声）到复杂数据分布（如真实图像）的连续变换**。这种变换不是离散的跳跃，而是通过一个连续的、可微分的动力学系统来描述。

演讲者（特别是何恺明和Ricky T. Q. Chen）通过形象的比喻来解释“流”：
*   **数据流形**：数据（如图像）存在于一个高维空间中的低维流形上。
*   **抽象与具象**：识别模型是从具象的数据流向抽象的嵌入空间，而生成模型则是从抽象的嵌入空间（或噪声）流向具象的数据流形。
*   **连续变换**：这种“流”可以被一个向量场（Vector Field）或生成器（Generator）来描述，它指导着数据点如何从一个状态平滑地过渡到另一个状态。

### 2. 什么是“流模型”（Flow Models）？

“流模型”是一类基于上述“流”概念构建的生成模型。它们旨在学习并参数化这种连续的变换过程，从而实现从简单噪声到复杂数据的生成。研讨中主要提到了两种广义上的流模型：

*   **归一化流（Normalizing Flows, NF）**：这是狭义上的“流模型”，也是本次研讨中Jiatao Gu（顾嘉涛）重点介绍的。
*   **基于流的扩散模型（Flow-based Diffusion Models）**：这是广义上的“流模型”，包括扩散模型本身以及流匹配（Flow Matching）等技术。

下面将详细阐述这两种流模型及其细分种类。

---

### 细分种类一：归一化流（Normalizing Flows, NF）

**基本概念：**
归一化流是一类通过**一系列可逆变换**将简单先验分布（如标准高斯分布）映射到复杂数据分布的生成模型。由于其变换是可逆的，因此可以精确计算数据点的概率密度，从而通过最大似然估计进行训练。

**核心原理：**
1.  **可逆变换**：模型由一系列可逆函数 $f_1, f_2, ..., f_T$ 组成，将噪声 $z$ 变换为数据 $x$ ($x = f_T \circ ... \circ f_1(z)$)。
2.  **变量变换公式（Change-of-Variables Formula）**：这是NF的数学基础。它允许我们计算数据 $x$ 的概率密度 $p(x)$：
    $p(x) = p_0(f(x)) \left| \det\left(\frac{\partial f(x)}{\partial x}\right) \right|^{-1}$
    其中，$p_0(z)$ 是先验分布的密度，$f(x)$ 是从数据到噪声的映射（即 $f = f_T \circ ... \circ f_1$ 的逆函数），$\left| \det\left(\frac{\partial f(x)}{\partial x}\right) \right|$ 是雅可比行列式的绝对值，它衡量了变换过程中体积的变化（“Local Volume Change”）。
3.  **最大似然训练**：由于可以精确计算 $p(x)$，NF可以直接通过最大化训练数据的对数似然 $\log p(x)$ 进行训练。

**传统NF的挑战（Jiatao Gu提到）：**
1.  **可逆性约束**：为了保持可逆性，NF的内部结构受到严格限制，这限制了其模型的表达能力。
2.  **雅可比行列式计算**：计算雅可比行列式及其行列式值通常非常昂贵，尤其是在高维空间中。
3.  **架构灵活性差**：由于上述限制，传统NF难以设计出像U-Net或Transformer那样灵活且强大的架构，导致其在处理高维图像数据时性能不佳。

**本次研讨中NF的复兴与创新（STARFlow）：**
Jiatao Gu团队通过STARFlow（Scalable Latent Normalizing Flows for High-resolution Image Synthesis）解决了传统NF的这些挑战，使其在高分辨率图像生成中表现出色。

*   **自回归流 (Autoregressive Flow)**：
    *   **原理**：利用自回归模型的特性（每个输出只依赖于之前的输出），使得变换的雅可比矩阵是三角矩阵，从而雅可比行列式可以高效计算（对角线元素的乘积）。
    *   **优势**：解决了雅可比行列式计算的难题，并自然地满足可逆性。
    *   **堆叠**：单个自回归流块表达能力有限（通常假设为高斯分布），但通过堆叠多个自回归流块，可以近似任意复杂的连续密度函数，并引入“未来信息”（通过后续块的输入）。
*   **Transformer骨干**：将NF中的变换函数现代化为Transformer架构，借鉴了DiT（Diffusion Transformer）的成功经验，提升了模型的表达能力。
*   **深浅层架构 (Deep-Shallow Architecture)**：
    *   **结构**：采用深层（处理语义信息）和浅层（处理局部细节）结合的架构。深层Transformer处理全局语义和条件信息，浅层Transformer处理局部像素细节。
    *   **优势**：提高了性能和效率，并能与预训练LLM进行微调结合。
*   **潜在空间归一化流 (Latent Normalizing Flows)**：
    *   **原理**：借鉴LDM（Latent Diffusion Models）的思路，在VAE的潜在空间中运行NF。
    *   **优势**：潜在空间维度更低，训练更稳定，且VAE解码器可以辅助去噪。
*   **噪声增强训练 (Noise-augmented Training)**：
    *   **问题**：传统NF的极大似然训练在连续空间中存在“零体积流形”问题，导致训练不稳定和样本质量差。
    *   **解决方案**：在数据点上添加少量固定噪声，使其具有非零体积，从而稳定训练并提高样本质量。
    *   **生成干净图像**：通过基于分数的去噪或利用VAE解码器进行去噪。
*   **带引导的推理 (Inference with Guidance)**：
    *   将扩散模型中成功的分类器自由引导（CFG）技术引入自回归流，并重新推导适用于潜在空间的引导公式。

---

### 细分种类二：基于流的扩散模型（Flow-based Diffusion Models）

这类模型将扩散过程本身视为一种连续的流，并在此基础上进行建模和优化。

**1. 扩散模型 (Diffusion Models) 的流视角：**
*   **原理**：扩散模型通过一个前向扩散过程（逐步向数据添加噪声）和一个反向去噪过程（逐步从噪声中恢复数据）来工作。
*   **连续流类比**：在连续时间极限下，扩散过程可以被描述为随机微分方程（SDE）或常微分方程（ODE）。生成过程就是求解这些方程，将简单噪声分布“流”向复杂数据分布。
*   **局限性（从流的视角看）**：
    *   **DDIM的次优性**：Jiaming Song指出，DDIM（Denoising Diffusion Implicit Models）在推理时只考虑单个时间步，模型对终点信息感知不足，导致其在多步推理时效率不高。
    *   **高曲率轨迹**：Arash Vahdat提到，扩散模型学习的轨迹可能存在高曲率，导致离散化误差，需要更多步骤才能准确模拟，从而降低采样效率。

**2. 流匹配 (Flow Matching, FM)：**
*   **核心概念**：流匹配是一种更通用的生成模型训练范式，它直接学习一个“速度场”（vector field），该速度场定义了从噪声分布到数据分布的确定性路径。扩散模型可以被视为流匹配的一个特例。
*   **优势**：流匹配的训练目标是直接回归这个速度场，相比于扩散模型的去噪分数匹配，其训练更稳定，且理论上能实现更高效的生成。
*   **挑战**：尽管理论上流匹配可以实现连续生成，但在实际计算中，仍然需要通过有限和（finite sum）来近似积分，这使得其在实践中仍类似于ResNet的离散更新。

**3. MeanFlow：**
*   **创新点**：MeanFlow旨在解决流匹配中的离散近似问题，实现真正的“一步生成”。它引入了“平均速度”（Average Velocity）的概念，将生成过程的积分计算转化为可导的导数计算（通过Jacobian-vector product, JVP），从而避免了复杂的迭代求解。
*   **与AlexNet的类比**：MeanFlow试图将生成模型从“逐步去噪”的范式（类似于AlexNet之前的逐层训练）推向“一步到位”的端到端生成，即直接学习从噪声到最终图像的映射，并直接优化最终输出。

**4. Generator Matching (GM)：**
*   **提出者**：Ricky T. Q. Chen团队。
*   **核心思想**：将流匹配和扩散模型中的“生成器”（Generator，即定义数据演化方向的函数）概念推广到更一般的马尔可夫过程。它允许生成器不仅包含连续的流（ODE/SDE），还可以包含**跳跃（Jumps）**。
*   **跳跃过程**：在每一步迭代中，模型可以选择在当前位置进行局部扰动（流），也可以选择“跳跃”到空间中完全不同的位置（由模型学习的未归一化分布指导）。这种跳跃机制使得模型能够处理不连续性、插入、删除和维度变化。
*   **训练**：GM的训练目标与流匹配类似，都是学习条件期望，但其损失函数可以采用更广义的Bregman散度，以适应不同类型的生成器（如处理离散数据）。
*   **优势**：
    *   **处理不连续性**：能更好地处理文本生成中常见的插入、删除、替换等操作，以及视觉中可能存在的非局部变化。
    *   **提高效率**：通过跳跃，模型可以更高效地探索状态空间，避免在低密度区域进行冗余的局部搜索。
*   **细分种类**：
    *   **离散流匹配 (Discrete Flow Matching)**：将GM应用于离散空间，通过学习未归一化的分布来指导跳跃，解决传统离散扩散模型中单token修改的局限性。
    *   **编辑流 (Edit Flows)**：将维度变化（插入和删除）引入流模型，实现可变长度序列的生成和编辑，如文本生成和代码生成。

---

### 总结：流模型家族的演进

本次研讨展示了“流”概念在生成模型中的广泛应用和深刻演进：

*   **从狭义到广义**：从最初的归一化流（Normalizing Flows）这种严格可逆的连续变换模型，扩展到将扩散过程也视为一种连续流（Flow-based Diffusion Models），再到更一般的马尔可夫过程（Generator Matching）。
*   **从连续到离散**：传统流模型主要处理连续数据，但GM和离散流匹配则将流的概念推广到离散空间，使其能够处理文本、代码等离散序列。
*   **从局部到全局**：通过引入跳跃机制，模型能够进行非局部的、更高效的状态转移，弥补了传统流模型和扩散模型在处理大范围、不连续变化时的不足。
*   **效率与控制**：所有这些流模型的新发展，都旨在提高生成效率（一步生成、更少采样步数）和生成过程的可控性，以应对“扩散模型之后”的挑战。

可以说，“流模型”已经从一个具体的模型类别，演变为一种更普遍的**建模数据动态演变和生成过程的范式**，为下一代生成模型的发展提供了丰富的理论基础和实践方向。

